@article{Miranda2021,
  abstract      = {This study analyzed the annual obligatory and traditional speeches, referred
to as State of the Nation Address (SONA), of the 13 past Philippine presidents. The study
determined the sentiments, as well as the emergent topics, expressed in these materials.
It is found that these SONAs generally expressed positive sentiments while the
lowest negative sentiment, on the other hand, was during the martial law period in
1974. Also, it is shown that “development” is the most frequently appeared word
among these speeches. The study also revealed that the sentiments of the incoming
presidents were lower than that of the outgoing. Moreover, it is shown that these
SONAs mainly focused on the following concerns of the country: (a) economic development;
(b) enhancement of public services; and (c) addressing challenges. The results
of the study translate into the importance of SONA as a venue to discuss and to engage
with its people the nation’s state and direction.},
  author        = {John Paul, P. Miranda & Rex P. Bringula},
  doi           = {10.1080/23311886.2021.1932030},
  issn          = {1932030},
  journal       = {Cogent Social Sciences},
  keywords      = {Data mining; Philippines; president; speech; text mining},
  month         = jul,
  title         = {{Exploring Philippine Presidents’ speeches: A sentiment analysis and topic modeling approach}},
  url           = {https://doi.org/10.1080/23311886.2021.1932030},
  volume        = {1},
  year          = {2021}
}

@article{Finity2020,
  abstract      = {Campaign speeches provide significant insight into how candidates communicate their message and highlight their priorities to various audiences.},
  author        = {Kevin Finity, Ramit Garg & Maxwell McGaw},
  journal       = {University of Virginia, School of Data Science},
  title         = {{A Text Analysis of the 2020 US Presidential Election Campaign Speeches}},
  url           = {https://datascience.virginia.edu/projects/text-analysis-2020-us-presidential-election-campaign-speeches},
  year          = {2020}
}

@misc{zhao2021topic,
      title={Topic Modelling Meets Deep Neural Networks: A Survey}, 
      author={He Zhao and Dinh Phung and Viet Huynh and Yuan Jin and Lan Du and Wray Buntine},
      year={2021},
      eprint={2103.00498},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NEURIPS2021_7b6982e5,
 author = {Shen, Dazhong and Qin, Chuan and Wang, Chao and Dong, Zheng and Zhu, Hengshu and Xiong, Hui},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {14681--14693},
 publisher = {Curran Associates, Inc.},
 title = {Topic Modeling Revisited: A Document Graph-based Neural Network Perspective},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7b6982e584636e6a1cda934f1410299c-Paper.pdf},
 volume = {34},
 year = {2021}
}


@article{baker_korhonen_pyysalo_2017, title={Cancer Hallmark Text Classification Using Convolutional Neural Networks}, url={https://www.repository.cam.ac.uk/handle/1810/270037}, DOI={10.17863/CAM.12420}, publisher={Apollo - University of Cambridge Repository}, author={Baker, S and Korhonen, A and Pyysalo, S}, year={2017} }

@article{Lai_Xu_Liu_Zhao_2015, title={Recurrent Convolutional Neural Networks for Text Classification}, volume={29}, url={https://ojs.aaai.org/index.php/AAAI/article/view/9513}, DOI={10.1609/aaai.v29i1.9513}, abstractNote={ &lt;p&gt; Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classification without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun}, year={2015}, month={Feb.} }

@article{CHEN2022102798,
title = {A comparative study of automated legal text classification using random forests and deep learning},
journal = {Information Processing & Management},
volume = {59},
number = {2},
pages = {102798},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102798},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002764},
author = {Haihua Chen and Lei Wu and Jiangping Chen and Wei Lu and Junhua Ding},
keywords = {Legal text classification, Machine learning, Deep learning, Domain concept, Word embedding, Random forests},
abstract = {Automated legal text classification is a prominent research topic in the legal field. It lays the foundation for building an intelligent legal system. Current literature focuses on international legal texts, such as Chinese cases, European cases, and Australian cases. Little attention is paid to text classification for U.S. legal texts. Deep learning has been applied to improving text classification performance. Its effectiveness needs further exploration in domains such as the legal field. This paper investigates legal text classification with a large collection of labeled U.S. case documents through comparing the effectiveness of different text classification techniques. We propose a machine learning algorithm using domain concepts as features and random forests as the classifier. Our experiment results on 30,000 full U.S. case documents in 50 categories demonstrated that our approach significantly outperforms a deep learning system built on multiple pre-trained word embeddings and deep neural networks. In addition, applying only the top 400 domain concepts as features for building the random forests could achieve the best performance. This study provides a reference to select machine learning techniques for building high-performance text classification systems in the legal domain or other fields.}
}

@article{SALLES20181,
title = {Improving random forests by neighborhood projection for effective text classification},
journal = {Information Systems},
volume = {77},
pages = {1-21},
year = {2018},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S030643791830156X},
author = {Thiago Salles and Marcos Gonçalves and Victor Rodrigues and Leonardo Rocha},
keywords = {Classification, Random forests, Lazy learning, Nearest neighbors},
abstract = {In this article, we propose a lazy version of the traditional random forest (RF) classifier (called LazyNN_RF), specially designed for highly dimensional noisy classification tasks. The LazyNN_RF “localized” training projection is composed by examples that better resemble the examples to be classified, obtained through nearest neighborhood training set projection. Such projection filters out irrelevant data, ultimately avoiding some of the drawbacks of traditional random forests, such as overfitting due to very complex trees, especially in high dimensional noisy datasets. In sum, our main contributions are: (i) the proposal and implementation of a novel lazy learner based on the random forest classifier and nearest neighborhood projection of the training set that excels in automatic text classification tasks, as well as (ii) a throughout and detailed experimental analysis that sheds light on the behavior, effectiveness and feasibility of our solution. By means of an extensive experimental evaluation, performed considering two text classification domains and a large set of baseline algorithms, we show that our approach is highly effective and feasible, being a strong candidate for consideration for solving automatic text classification tasks when compared to state-of-the-art classifiers.}
}


