---
author:
  - name: Steven Ellis
    email: ELLSTE005@uct.ac.za
        
title: Predictive Modelling of Presidential SONA Addresses in South Africa from 1994 to 2023
keywords: SONA, text modelling, sentiment analysis, latent Dirichlet allocation, LDA, bag-of-words
abstract: |
  In this assignment, I was assigned producing at least three predictive models that take a sentence of text as input and return return a prediction of which South African president was the source of that sentence. The data-set for the assignment was all SONA speeches delivered between 1994 and 2023. 
  
  After import, transformation and cleaning of the speech data, the issue of imbalanced data was addressed. Thereafter the data was tokenized and modelled into two separate spare-and-wide formats for predictive modelling - bag-of-words and tf-idf. Four predictive models were then generated -a feed-forward neural network, a classification tree, a random forest and a convolutional neural network. These were tuned, fitted and evaluated against a subset of unseen data to guage performance. After assessing model results, it was concluded that the feed-forward neural network produced the best overall predictive accuracy.
bibliography: bibliography.bib
format:
  html:
      toc: true
      toc-location: left
      toc-title: Contents
      embed-resources: true
      number-sections: true
      fig-pos: '!ht'
  
---

```{r libraries, message=FALSE, warning=FALSE, results='hide', echo=FALSE}
library(knitr)
library(tidyverse)
library(textstem)
library(tidytext)
```

```{r extract, message=FALSE, warning=FALSE, results='hide', echo=FALSE}

load("sona.RData")
load("dataset_splits.RData")
load("neural_networks_results.RData")
load("classification_tree_results.RData")
load("random_forest_results.RData")
load("convolutional_neural_results.RData")

```

## Introduction

The State of the Nation Address of the President of South Africa is an annual event in the Republic of South Africa, in which the President of South Africa reports on the status of the nation, normally to the resumption of a joint sitting of Parliament. 

This assignment generated four predictive models that attempt to predict from a sentence the South African president who delivered it. The data-set provided included 36 speeches from 1994 to 2023 delivered by six different presidents. 

A review of academic articles revealed that the use of neural networks for topic modelling and text analysis has been identified as a fast-growing research area. [@zhao2021topic] [@NEURIPS2021_7b6982e5]. 

Similarly, the use of convolutional neural networks for text classification has shown that basic CNN models can achieve very good levels of text classification performance when compared to other models such as Support Vector Machines [@baker_korhonen_pyysalo_2017] [@Lai_Xu_Liu_Zhao_2015]

Random forests (described later) have also demonstrated effective text classification capabilities, in some cases outperforming traditional deep-learning machine learning techniques traditionally used in this domain. [@CHEN2022102798] [@SALLES20181]

For this assignment, the speech data was imported, cleaned and transformed into word-based tokens. 
Since not all presidents had the same amount of speech content in the final data-set, the data-set was then balanced using various techniques, described in detail in the next section.

Four models - a *feed-forward neural network*, a *classification tree*, a *random forest* and a *convolutional neural network* - were then run against the data-sets and thereafter individually assessed. 


## Exploratory Data Analysis

From a review of different SONA speeches, it was evident that the style and diction of SONA speeches, across years and presidents, is remarkably similar. 
The speeches generally follow a set format, with common words, phrases and remarks permeating throughout. 
As seen in @fig-eta below, certain words are very frequently used in SONA speeches.

```{r fig-eta, message=FALSE, warning=FALSE, results='hide', echo=FALSE, fig.align='center', fig.cap="Top 20 words used by all presidents.", fig.width = 10}

sona_eta <- sona %>% mutate(speech = lemmatize_strings(tolower(speech)))
unnest_reg <- "[^\\w_#@']"

sona_eta %>% 
  unnest_tokens(word, speech, token = 'regex', pattern = unnest_reg) %>%
  filter(str_detect(word, '[a-z]')) %>% 
  filter(!word %in% stop_words$word) %>%
  count(word, sort = T) %>%
  slice(1:20) %>%
  ggplot(aes(x = reorder(word, n), y = n, fill = -n)) + geom_col() + coord_flip() + ylab('Count') + xlab('') +
  theme(plot.title = element_text(size = 7), 
        axis.title.x = element_text(size = 7), 
        axis.title.y = element_text(size = 7),
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 7),
        legend.position = '')+
  scale_fill_gradient(low = "red", high = "blue") 

```

A review of the number of speeches delivered per president revealed that the data-set was very imbalanced, as shown in @fig-eta2 below, with two presidents (deKlerk and Motlanthe) only delivering one speech each. 

```{r fig-eta2, message=FALSE, warning=FALSE, results='hide', echo=FALSE, fig.align='center', fig.cap="Total Number of Speeches per President.", fig.width = 8}

speech_counts <- sona %>%
  group_by(pres) %>%
  summarise(total_speeches = n()) %>%
  arrange(total_speeches)

barplot(speech_counts$total_speeches, 
        names.arg = speech_counts$pres,
        xlab = "President",
        col = "skyblue",
        ylab = "Total Speeches")

```

The average number of sentences per speech delivered also varied between presidents, with deKlerk's average significantly lower than the other presidents', as shown in @fig-eta3 below.

```{r fig-eta3, message=FALSE, warning=FALSE, results='hide', echo=FALSE, fig.align='center', fig.cap="Average Number of Sentences per President.", fig.width = 10}

speech_avg <- sentences$unbalanced %>% mutate(year = as.integer(speech_year)) %>%
  group_by(pres) %>%
  summarise(average_speeches = n() / n_distinct(year))  %>%
  arrange(average_speeches)

barplot(speech_avg$average_speeches, 
        names.arg = speech_avg$pres,
        xlab = "President",
        col = "skyblue",
        ylab = "Average Sentences")

```

It was evident that the commonality of words across speeches and the imbalanced data would both need to be addressed before predictive modelling could be performed.

## Data and Methods

### Data import and transformation

As part of the process of importing, cleaning and pre-processing the data, the following tasks were performed:

* All speeches were imported from their respective `.txt` files into a single data-frame
* The speech *year* and *president* were added as columns to the data-frame
* Any links or non-ASCII words were removed from the sentences via the **str_replace_all()** function from the R `stringr` library
* The data-frame was then converted to a tibble

### Imbalanced data

From the EDA section above, it can be seen that two presidents (deKlerk and Motlanthe) only delivered one SONA speech, meaning their words and sentences were extremely under-represented in the data-set. 
In addition, some presidents had more average sentences per speech than others. 

Since imbalanced data can have an adverse effect on predictive modelling, it was decided that *three* different data-sets would be produced for predictive modelling:

1. An **imbalanced** data-set, where presidential speeches were left in their *original proportions*
2. A **under-sampled** data-set: in this data-set, the speeches the two presidents with only one speech (*deKlerk* and *Motlanthe*) were removed. On the remaining data-set, under-sampling was performed by limiting each president's sentences to the *minimum* number of sentences delivered by a president within the remaining consort (1665 sentences per president)
3. An **over-sampled** data-set. Over-sampling involves adding more samples from under-represented classes. To achieve this, the `oversample_smote()` function by the R `scutr` library was used to create synthetic samples from under-represented presidential sentences, with the end result that each president had an equal number of speeches in the final over-sampled data-set.

In order to prepare the data-set into one against which various predictive models could be run, the data-set needed to be tokenized and transformed from 'long' to 'wide' format, where each word (and the frequency count of how often it was represented in a sentence delivered by a president) was represented as a numeric column in a sparse and wide data-set. 

To achieve this, the data-set was first tokenized into **sentences**, using the `unnest_tokens()` function from the R `tidytext` library, which split the speeches column into sentence tokens, flattening the resulting data-set into one-sentence-per-row.

Thereafter, two methodologies were used to **word-tokenize** the resulting data-set: the *bag-of-words* method and *term frequency-inverse-document-frequency (tf-idf)* method. 

### Bag-of-words

In a bag-of-words model, a document corpus is represented by the frequency counts of the words used therein, via a simplified representation that ignores word order and grammar. 

To achieve a bag-of-words mode against the sentence tokens, all sentences had stop-words removed, and were then tokenized per word. 
From the tidy word-tokenized data-set, the top **200** most widely-used words were extracted. 
Thereafter, the number of times each of these 200 words was used in each sentence was calculated.
Finally, the resulting data-set was re-shaped using R `pivot_wider()` function, so that each *sentence* was in its own row, and each *word* in its own column (with each column value representing the word's frequency count for that row's sentence). 

This wide, sparse and untidy data-set was then ready for predictive modelling.

The bag-of-words transformation was performed on all three data-sets (*imbalanced*, *under-sampled* and *over-sampled*).  


### Term frequency-inverse-document-frequency (TF-IDF)

What is clear from reading some of the SONA speeches is that certain words and phrases are habitually repeated by all presidents. 
In order to assist with predictive modelling, a common technique is to down-weigh words in a term that are used frequently by all presidents (such as 'deliver', 'budget', 'economy', 'invest') and up-weigh words that are more frequently used only by a single president. 

This is what tf-idf aims to achieve. It calculates a inverse-document-frequency-weighted-term-frequencies score for each word, which is low for words commonly used in many documents, and higher for words that are not used by many documents in a corpus. 

The tf-idf scores for each word were calculated using the `bind_tf_idf()` function from the R `tidytext` library.

The tf-idf transformation was also performed on all three data-sets (*imbalanced*, *under-sampled* and *over-sampled*).

### Test and training data-sets

Once Bag of Words and tf-idf transformations were completed on the three data-sets, they were split into training and testing sets. 
The split chosen was a **70/30** split, where 70% of the observations were used to train predictive models and 30 were used for testing. 

On both neural network and convolutional neural network models, a **10%** validation set was used during model fitting.


### Model 1: Neural Network

Neural networks are a subset of machine learning and are at the heart of deep learning algorithms.
These models are comprised of a node layers, containing an *input* layer, one or more *hidden* layers, and an *output* layer. 

Each node, or artificial neuron, connects to another and has an associated *weight* and *threshold*. Weights are similar to the coefficients used in regression equations.

The weighted inputs are summed and passed through an *activation function* (which simply maps the summed weighted inputs to the output of the neuron, typically using a non-linear function). 
If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.
 
This topology and methodology allows the network to combine the inputs in very complex ways, and thus to model highly non-linear data-sets.

For this assignment, the R `keras` library was used, and the `keras_model_sequential()` function was used to create a linear stack of layers in our deep learning model. 
Keras was used to assemble layers into a fully-connected multi-layer perceptron. After several rounds of hyper-parameter testing, the following specifications were chosen for the model:

* 64 units in input layer, with *ReLu* activation function
* dropout layer with a rate of 0.1 
* 128 units in hidden layer with *ReLu* activation function 
* dropout layer with a rate of 0.2
* 6 units in output layer with *softmax* function

The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training, helping to prevent over-fitting. The *softmax* activation function was chosen for the output layer because of the multi-class classification requirement.

To prepare the data-set for multi-class classification, one hot encoding was performed on the target attribute target using the `to_categorical()` function in R `keras` library.

The **Adam** optimiser was selected, and the **categorical_crossentropy** loss function was used (because of the multi-class classification requirement), and the chosen metric was **accuracy**. 

When fitting the model a batch_size of *128* was applied and *50* epochs were chosen. 
A *10%* validation split was used during model fitting.


### Model 2: Classification Tree

Classification trees are used for data classification through a process known as binary recursive partitioning. This is an iterative process of splitting the data into partitions, and then splitting it up further on each of the branches.

Classification trees are also the fundamental components to random forests (also used in this assignment). Their advantages include good interpretability (being easy to visualise) and they do not require the data to be processed before classification modelling. Their disadvantages are that they are prone to over-fitting, and they cannot guarantee global optimisation (because they are built in a greedy manner).  

To generate classification trees models the `rpart()` R function was used from the `rpart` library.

### Model 3: Random Forest

While decision trees are common supervised learning algorithms, they can be prone to bias and over-fitting. The Random Forest model strives to overcome this weakness, by combining the output of multiple decision trees to reach a single result. 

Random forests ensure that the decision trees are *de-correlated* as follows: when building decision trees in a random forest, each time a split in a tree is considered, a random sample of *m* predictors is chosen as split candidates from the full set of *p* predictors. This prevents dominant predictors giving all trees in the model correlated results. 

To generate random forest classification models, the `randomForest()` R function was used. After several rounds of testing, a forest of *150* trees was selected. 


### Model 4: Convolutional Neural Network

The Convolutional Neural Network is a regularized type of feed-forward neural network, that learns feature engineering by itself via filters optimization. It is typically used for image and media recognition. 
CNNs use four key processes for predictive modelling:

1. *Convolution*: the CNN scans the input variables (usually in matrix format) with window-like filters to detect features in the data (for example colours or edges in images).
2. *Feature maps*: From the features found in step 1, maps are generated. For image data, such a map might highlight edges, while another might focus on colors.
3. *Pooling*: the CNN next reduces the size of these feature maps by keeping the most important information and discarding the rest. 
4. *Fully connected layers*: the network then flattens the maps and uses them for predictive modelling, detecting patterns from these flattened layers.

To generate the CNN models, the `text_tokenizer()` and `fit_text_tokenizer()` functions from `keras` was used to vectorize the top X words from a balanced and unbalanced data-sets. 
These were then padded to equal length before being fitted to a convolutional network via the `keras_model_sequential()` function.
The exercise was repeated for top *100*, *200* and *300* words.

One hot encoding was again performed on the target attribute target using the `to_categorical()` function in R `keras` library.

After several rounds of hyper-parameter testing, the following network toplogy was selected: 

* embedding layer to convert the data into dense vectors
* dropout layer of rate 0.1 
* convolution layer (layer_conv_1d) with *ReLu* activation function, containing 64 filters and a kernel size of 8
* maxpooling1D layer of size 2
* densely-connected NN layer of dimensionality 32, with *ReLu* activation function
* densely-connected NN layer of dimensionality 7, with *Softmax* activation function

The **Adam** optimiser was used, and the **categorical_crossentropy** loss function was used, and the chosen metric was **accuracy**. 

When fitting the model a batch_size of *128* was applied and *50* epochs were chosen. A validation split of *10%* was again used during model fitting.


## Results

### Classification Tree

```{r tbl-classification-tree, echo=FALSE}
#| label: tbl-classification-tree
#| tbl-cap: "Classification Tree Model Prediction Accuracies"
#| tbl-colwidths: [60,20,20]

kable(classification_tree_results)
```

From the results in @tbl-classification-tree it is clear that the classification tree model generally performed poorly against all data-sets.

```{r tbl-classification-tree-pred, echo=FALSE}
#| label: tbl-classification-tree-pred
#| tbl-cap: "Classification Tree Model Predictions Against **Unbalanced TF-IDF** Data-Set"

kable(classification_tree_tfidf_unbalanced_prediction)
```

Looking at the confusion matrix for the classification tree model against the unbalanced tf-idf data-set in @tbl-classification-tree-pred above, it was evident that the model did an poor job of predicting presidential speeches, with the majority of the predictions being assigned to two presidents (Mbeki and Zuma).  

### Feed-Forward Neural Network
 
```{r tbl-neural, echo=FALSE}
#| label: tbl-neural
#| tbl-cap: "Neural Network Model Prediction Accuracies"
#| tbl-colwidths: [60,20,20]

kable(neural_networks_results)
```

As can be seen in @tbl-neural above, the feed-forward neural-network performed well against the over-sampled data-sets. 

```{r tbl-conf1, echo=FALSE}
#| tbl-cap: "Neural Network - Confusion Matrix Against **Under-Sampled Bag-of-Words** Data-Set"

kable(neural_networks_bag_of_words_balanced_prediction)
```

When looking at the **confusion matrix** of the model used against the under-sampled bag-of-words data-set in @tbl-conf1, it is evident that whilst the diagonal represents the highest number for each row, the model did also suffer from a reasonably high proportion of incorrect predictions. 

```{r tbl-conf3, echo=FALSE}
#| tbl-cap: "Neural Network - Confusion Matrix Against **Over-Sampled TF-IDF** Data-Set"

kable(neural_networks_tfidf_oversampled_prediction)
```

The confusion matrix for the over-sampled tf-idf data-set in @tbl-conf3 reveals that the model achieved very high predictive accuracy against *deKlerk* and *Motlanthe*, both of whom where heavily over-sampled because of the fact that they delivered only one SONA speech. 
Nonetheless, the remaining presidents also exhibit better prediction rates when compared to the previous two confusion matrices above, suggesting that the over-sampling method was effective in balancing out the data-set for predictive modelling. 


### Random Forest

```{r tbl-random-forest, echo=FALSE}
#| label: tbl-random-forest
#| tbl-cap: "Random Forest Model Prediction Accuracies"
#| tbl-colwidths: [60,20,20]

kable(random_forest_results)
```

As seen in @tbl-random-forest, the random forest classifier produced satisfactory results across all data-sets, achieving over 49% accuracy against the over-sampled bag-of-words data-set.

```{r tbl-rf-pred, echo=FALSE}
#| label: tbl-rf-pred
#| tbl-cap: "Random Forest Model Predictions Against **Oversampled TF-IDF** Data-Set"

colnames(random_forest_bag_of_words_oversampled_prediction) = c('Mandela', 'Mbeki', 'Ramaphosa', 'Zuma', 'deKlerk', 'Motlanthe')
rownames(random_forest_bag_of_words_oversampled_prediction) = c('Mandela', 'Mbeki', 'Ramaphosa', 'Zuma', 'deKlerk', 'Motlanthe')
kable(random_forest_bag_of_words_oversampled_prediction)
```

The confusion matrix of the random forest classifier in @tbl-rf-pred, however, shows predictions skewed towards Zuma, except for the heavily over-sampled deKlerk and Motlanthe.

### Convolutional Neural Network

```{r tbl-cn, echo=FALSE}
#| label: tbl-cn
#| tbl-cap: "Convolutional Neural Network Model Prediction Accuracies"
#| tbl-colwidths: [60,20,20]

kable(cnn_results)
```

Finally, as shown in @tbl-cn, the CNN classifier also produced satisfactory results across all data-sets, achieving best results of almost 46% accuracy.

```{r tbl-cnn-pred, echo=FALSE}
#| label: tbl-cnn-pred
#| tbl-cap: "CNN Model Predictions Against **Under-Sampled TF-IDF** Data-Set - Top 300 Words"


colnames(cnn_results_prediction_undersampled_300) = c('Mandela', 'Mbeki', 'Ramaphosa', 'Zuma')
rownames(cnn_results_prediction_undersampled_300) = c('Mandela', 'Mbeki', 'Ramaphosa', 'Zuma')
kable(cnn_results_prediction_undersampled_300)

```

The confusion matrix of the CNN classifier against the under-sampled tf-idf data-set in @tbl-cnn-pred shows relatively good prediction rates except for Zuma.

## Discussion & Conclusion

The assignment required building three or more predictive models that, that given a sentence of text, could predict which
president was the source of that sentence. The task was performed by importing and cleaning the data, then tokenizing the data and converting it into a bag-of-words and tf-idf formats. 

Given the imbalanced state of the data-set, the decision was made to create three different data-sets for comparison - an imbalanced, an under-sampled and an over-sampled data-set. Thereafter the three data-sets were split into training and test sets, and the models built and tuned for classification prediction.

Of the four models, the feed-forward neural network produced the best predictive results, although with the caveat that it was against the over-sampled data-set, which produced very accurate predictions against presidents with only one SONA speech. 
The CNN model also produced satisfactory results, followed by the random forest, and the classification tree last. 

What was evident is that the imbalanced data-set and commonly-used set-of-words posed challenges to the models, perhaps suggesting the relatively low rates of accuracy. Nonetheless, when compared to random chance or a naive model (where the same president is always selected), all models were superior. Both such models, over sufficient repeated runs, would produce a best-case approximate success rate of 17% against both balanced and unbalanced data, which is inferior to even the worst performing model. 

Possible future improvements could be the investigation of other predictive models (for example Gradient Boosted Machines), and more intensive hyper-parameter tuning. 
In addition, the bag-of-words and tf-idf models could be enhanced to look at a larger number of words (for example 500), to evaluate performance, assuming sufficient computational resources to do so.

## References
